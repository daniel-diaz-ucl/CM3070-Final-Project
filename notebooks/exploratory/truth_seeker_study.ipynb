{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307a7103",
   "metadata": {},
   "source": [
    "## Final Project for CM3070\n",
    "\n",
    "This is an exploratory notebook into the dataset `TruthSeeker2023`. Our first step involves importing the necessary libraries to facilitate our analysis and insights extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6b22",
   "metadata": {},
   "source": [
    "Load the dataset into a pandas Dataframe and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7521bfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders - 6 Month Update\\n\\nInfl...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@S0SickRick @Stairmaster_ @6d6f636869 Not as m...</td>\n",
       "      <td>NO MAJORITY</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>THE SUPREME COURT is siding with super rich pr...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders\\n\\nBroken campaign promi...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@OhComfy I agree. The confluence of events rig...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      author                                          statement  \\\n",
       "0           0  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "1           1  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "2           2  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "3           3  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "4           4  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "\n",
       "   target  BinaryNumTarget                 manual_keywords  \\\n",
       "0    True              1.0  Americans, eviction moratorium   \n",
       "1    True              1.0  Americans, eviction moratorium   \n",
       "2    True              1.0  Americans, eviction moratorium   \n",
       "3    True              1.0  Americans, eviction moratorium   \n",
       "4    True              1.0  Americans, eviction moratorium   \n",
       "\n",
       "                                               tweet 5_label_majority_answer  \\\n",
       "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...            Mostly Agree   \n",
       "1  @S0SickRick @Stairmaster_ @6d6f636869 Not as m...             NO MAJORITY   \n",
       "2  THE SUPREME COURT is siding with super rich pr...                   Agree   \n",
       "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...            Mostly Agree   \n",
       "4  @OhComfy I agree. The confluence of events rig...                   Agree   \n",
       "\n",
       "  3_label_majority_answer  \n",
       "0                   Agree  \n",
       "1                   Agree  \n",
       "2                   Agree  \n",
       "3                   Agree  \n",
       "4                   Agree  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path to the dataset\n",
    "dataset_path = '../../datasets/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv'\n",
    "\n",
    "# Load the dataset into a pandas dataframe, ensuring the header is inferred from the first row\n",
    "df = pd.read_csv(dataset_path, header=0)\n",
    "\n",
    "# Display the first five rows of the dataframe to inspect it\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98330dd",
   "metadata": {},
   "source": [
    "Counting columns, target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b397b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the dataframe: 9\n",
      "Count of different values in 'target' column:\n",
      "target\n",
      "True     68930\n",
      "False    65268\n",
      "Name: count, dtype: int64\n",
      "Count of different values in 'BinaryNumTarget' column:\n",
      "BinaryNumTarget\n",
      "1.0    68930\n",
      "0.0    65268\n",
      "Name: count, dtype: int64\n",
      "bool\n"
     ]
    }
   ],
   "source": [
    "# Display the number of columns in the dataframe\n",
    "print(f\"Number of columns in the dataframe: {len(df.columns)}\")\n",
    "\n",
    "# Show a count of different values available in the 'target' column\n",
    "print(\"Count of different values in 'target' column:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Show a count of different values available in the next column\n",
    "print(f\"Count of different values in 'BinaryNumTarget' column:\")\n",
    "print(df['BinaryNumTarget'].value_counts())\n",
    "\n",
    "print(df['target'].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d758fa1",
   "metadata": {},
   "source": [
    "It seems to be a balanced dataset! That is promising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52dd01",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we perform some data preprocessing on our dataframe `df`. The dataframe contains several columns, among which `target`, `5_label_majority_answer`, and `3_label_majority_answer` are of our interest.\n",
    "\n",
    "The `target` column contains boolean values (True/False), `5_label_majority_answer` can have five different values: 'agree', 'mostly agree', 'disagree', 'mostly disagree' and 'NO MAJORITY', and `3_label_majority_answer` can have three different values: 'agree', 'disagree' and 'unrelated'.\n",
    "\n",
    "We first filter out the rows where `5_label_majority_answer` is 'NO MAJORITY' or `3_label_majority_answer` is 'unrelated'. This is done to focus our analysis on the rows where we have a clear majority answer.\n",
    "\n",
    "Next, we create a new column `categorical_label` based on the values in `target` and `3_label_majority_answer` columns. The value in `categorical_label` is True if `target` is True and `3_label_majority_answer` is 'agree', or if `target` is False and `3_label_majority_answer` is 'disagree'. Otherwise, the value in `categorical_label` is False.\n",
    "\n",
    "Finally, we create another column `BinaryNumLabel` which is a numerical representation of `categorical_label`. The value in `BinaryNumLabel` is 1.0 if `categorical_label` is True, and 0.0 if `categorical_label` is False. This will be useful for any machine learning algorithms that require numerical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27d7c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "      <th>categorical_label</th>\n",
       "      <th>BinaryNumLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders - 6 Month Update\\n\\nInfl...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>THE SUPREME COURT is siding with super rich pr...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders\\n\\nBroken campaign promi...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@OhComfy I agree. The confluence of events rig...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>I've said this before, but it really is incred...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      author                                          statement  \\\n",
       "0           0  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "2           2  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "3           3  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "4           4  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "5           5  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "\n",
       "   target  BinaryNumTarget                 manual_keywords  \\\n",
       "0    True              1.0  Americans, eviction moratorium   \n",
       "2    True              1.0  Americans, eviction moratorium   \n",
       "3    True              1.0  Americans, eviction moratorium   \n",
       "4    True              1.0  Americans, eviction moratorium   \n",
       "5    True              1.0  Americans, eviction moratorium   \n",
       "\n",
       "                                               tweet 5_label_majority_answer  \\\n",
       "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...            Mostly Agree   \n",
       "2  THE SUPREME COURT is siding with super rich pr...                   Agree   \n",
       "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...            Mostly Agree   \n",
       "4  @OhComfy I agree. The confluence of events rig...                   Agree   \n",
       "5  I've said this before, but it really is incred...                   Agree   \n",
       "\n",
       "  3_label_majority_answer  categorical_label  BinaryNumLabel  \n",
       "0                   Agree               True             1.0  \n",
       "2                   Agree               True             1.0  \n",
       "3                   Agree               True             1.0  \n",
       "4                   Agree               True             1.0  \n",
       "5                   Agree               True             1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, eliminate from the dataframe every row that has in the corresponding column the value of NO MAJORITY, or the value of unrelated.\n",
    "df = df[(df['5_label_majority_answer'] != 'NO MAJORITY') & (df['3_label_majority_answer'] != 'unrelated')]\n",
    "\n",
    "# Function to calculate categorical_label\n",
    "def calculate_categorical_label(row):\n",
    "    label = row['3_label_majority_answer'].lower()  # convert to lowercase\n",
    "    if row['target'] == True and label == 'agree':\n",
    "        return True\n",
    "    elif row['target'] == True and label == 'disagree':\n",
    "        return False\n",
    "    elif row['target'] == False and label == 'agree':\n",
    "        return False\n",
    "    elif row['target'] == False and label == 'disagree':\n",
    "        return True\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['categorical_label'] = df.apply(calculate_categorical_label, axis=1)\n",
    "\n",
    "# Create BinaryNumLabel column\n",
    "df['BinaryNumLabel'] = df['categorical_label'].apply(lambda x: 1.0 if x == True else 0.0)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82874a50",
   "metadata": {},
   "source": [
    "Quick check of the remainder rows and whether is still a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eaf144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of different values in 'categorical_label' column:\n",
      "categorical_label\n",
      "True     57429\n",
      "False    54164\n",
      "Name: count, dtype: int64\n",
      "Count of different values in 'BinaryNumLabel' column:\n",
      "BinaryNumLabel\n",
      "1.0    57429\n",
      "0.0    54164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show a count of different values available in the 'target' column\n",
    "print(\"Count of different values in 'categorical_label' column:\")\n",
    "print(df['categorical_label'].value_counts())\n",
    "\n",
    "# Show a count of different values available in the next column\n",
    "print(f\"Count of different values in 'BinaryNumLabel' column:\")\n",
    "print(df['BinaryNumLabel'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed8667",
   "metadata": {},
   "source": [
    "Preprocessing of Tweets\n",
    "Preprocessing is a crucial step in any Natural Language Processing (NLP) task, including text classification of tweets. The goal of preprocessing is to clean and standardize the data before it is input into a model. Here are the preprocessing steps we used:\n",
    "\n",
    "Lowercasing: We convert all the text to lowercase. This helps to avoid having multiple copies of the same words (e.g., ‘Hello’, ‘hello’, ‘HELLO’).\n",
    "Handling URLs: We remove URLs as they may not provide useful information for our task. URLs are identified using regular expressions and are removed from the tweets.\n",
    "Handling User Mentions: User mentions (i.e., strings starting with ‘@’) are removed as they might not provide useful information for our task.\n",
    "Handling Hashtags: Hashtags (i.e., strings starting with ‘#’) can be tricky. They often convey important information in tweets. We keep the text of the hashtag but remove the ‘#’ symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d389c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r\"http\\\\S+|www\\\\S+|https\\\\S+\", '', text, flags=re.MULTILINE)  # remove urls\n",
    "    text = re.sub(r'\\\\@\\\\w+|\\\\#','', text)  # remove mentions and hashtags\n",
    "    return text\n",
    "\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20303dcf",
   "metadata": {},
   "source": [
    "Use of Classical ML Classifiers\n",
    "We use several classical machine learning classifiers to perform the task of text classification on our preprocessed tweets. These classifiers include:\n",
    "\n",
    "Random Forest: This is an ensemble learning method that operates by constructing multiple decision trees at training time and outputting the class that is the mode of the classes of the individual trees.\n",
    "Naive Bayes: This is a probabilistic classifier based on applying Bayes’ theorem with strong independence assumptions between the features.\n",
    "Logistic Regression: Despite its name, this is a linear model for classification rather than regression. It is also known as logit regression, maximum-entropy classification (MaxEnt), or the log-linear classifier.\n",
    "Gradient Boosting Classifier: This is another ensemble machine learning algorithm that constructs new predictors that aim to correct the residuals errors of the prior predictor, hence improving the model’s accuracy.\n",
    "Each of these classifiers is trained on the preprocessed tweet data and then used to predict the labels of the test data. The performance of each classifier is evaluated using standard metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ea2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Vectorize the tweets using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(df['processed_tweet'])\n",
    "y = df['BinaryNumLabel']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_jobs=-1),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {title}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate the classifiers\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(f\"\\\\n{name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    plot_confusion_matrix(y_test, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70720f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab7c10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0891f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e177912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running BERT without running Classical ML algorithms first\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3f2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the TweetDataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].view(-1)\n",
    "        attention_mask = encoding['attention_mask'].view(-1)\n",
    "\n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdd5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb7cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.diaz/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the tweets\n",
    "def preprocess(tweet):\n",
    "    return tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128, # from 512 -> 256\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "X = [preprocess(tweet) for tweet in df['tweet']]\n",
    "y = df['BinaryNumLabel'].tolist()\n",
    "\n",
    "# Split the data into training+validation and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training+validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Create the TweetDataset\n",
    "train_dataset = TweetDataset(\n",
    "    tweets=X_train,\n",
    "    labels=y_train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128 # from 512 -> 256\n",
    ")\n",
    "\n",
    "val_dataset = TweetDataset(\n",
    "    tweets=X_val,\n",
    "    labels=y_val,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128 # from 512 -> 256\n",
    ")\n",
    "\n",
    "test_dataset = TweetDataset(\n",
    "    tweets=X_test,\n",
    "    labels=y_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128 # from 512 -> 256\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing sets\n",
    "train_data = DataLoader(train_dataset, batch_size=32) # from 16 -> 32\n",
    "val_data = DataLoader(val_dataset, batch_size=32) # from 16 -> 32\n",
    "test_data = DataLoader(test_dataset, batch_size=32) # from 16 -> 32\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 10 # or any other number depending on how it goes\n",
    "\n",
    "# to check: early stopping\n",
    "# set a relatively high number of epochs (e.g., 100 or 1000), and then \n",
    "# stop the training early if the model’s performance on a validation set \n",
    "# stops improving after a certain number of epochs\n",
    "\n",
    "# Define the early stopping criteria\n",
    "patience = 3  # number of epochs to wait for improvement before stopping\n",
    "best_loss = None\n",
    "no_improve_count = 0\n",
    "\n",
    "# Initialize a list to hold the validation losses\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8f9f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a90f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/skqrkws14bs48tkyfrn2wnp80000gn/T/ipykernel_2345/3454705677.py:34: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'labels': torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training after 4 epochs due to no improvement.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = 0\n",
    "    for batch in val_data:  # use val_data instead of test_data\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "    # Add the validation loss for this epoch to the list\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_loss is None or val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        # Save the model\n",
    "        model_name = '_bert-base-uncased_'\n",
    "        dataset_name = '_TruthSeeker2023_'\n",
    "        model_path = f\"_model_epoch{epoch}_valLoss{val_loss:.2f}_batchSize{train_data.batch_size}.pth\"\n",
    "        torch.save(model.state_dict(), dataset_name + model_name + model_path)\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "\n",
    "    # Stop training if there's no improvement for 'patience' epochs\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Stopping training after {epoch} epochs due to no improvement.\")\n",
    "        break\n",
    "\n",
    "# Plot the validation loss over each epoch\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58593d44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366a22b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/skqrkws14bs48tkyfrn2wnp80000gn/T/ipykernel_2345/3454705677.py:34: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'labels': torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8776378870021059\n",
      "Precision: 0.8778171133667642\n",
      "Recall: 0.8776378870021059\n",
      "F1 Score: 0.8775543465560611\n"
     ]
    }
   ],
   "source": [
    "# code for model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get the predictions for the test set\n",
    "y_pred = []\n",
    "for batch in test_data:\n",
    "    input_ids = batch['input_ids'].squeeze().to(device)\n",
    "    attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    y_pred.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8dce6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2d30e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90793c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "268.353603284806\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print (epoch)\n",
    "print (val_loss)\n",
    "print (train_data.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebab7d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.defaults['lr'])\n",
    "print(optimizer.defaults['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7370f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = '_bert-base-uncased_'\n",
    "dataset_name = '_TruthSeeker2023_'\n",
    "model_path = f\"_model_epoch{epoch}_valLoss{val_loss:.2f}_batchSize{train_data.batch_size}.pth\"\n",
    "torch.save(model.state_dict(), dataset_name + model_name + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed84378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72dc0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the TweetDataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].view(-1)\n",
    "        attention_mask = encoding['attention_mask'].view(-1)\n",
    "\n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e112a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf2cb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.diaz/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the tweets\n",
    "def preprocess(tweet):\n",
    "    return tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,  # Changed from 512 to 256\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "X = [preprocess(tweet) for tweet in df['tweet']]\n",
    "y = df['BinaryNumLabel'].tolist()\n",
    "\n",
    "# Split the data into training+validation and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training+validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Create the TweetDataset\n",
    "train_dataset = TweetDataset(\n",
    "    tweets=X_train,\n",
    "    labels=y_train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512  # Changed from 512 to 256\n",
    ")\n",
    "\n",
    "val_dataset = TweetDataset(\n",
    "    tweets=X_val,\n",
    "    labels=y_val,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512  # Changed from 512 to 256\n",
    ")\n",
    "\n",
    "test_dataset = TweetDataset(\n",
    "    tweets=X_test,\n",
    "    labels=y_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512  # Changed from 512 to 256\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing sets\n",
    "train_data = DataLoader(train_dataset, batch_size=32)  # Changed from 16 to 32\n",
    "val_data = DataLoader(val_dataset, batch_size=32)  # Changed from 16 to 32\n",
    "test_data = DataLoader(test_dataset, batch_size=32)  # Changed from 16 to 32\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) #, weight_decay=0.01)  # Added weight_decay\n",
    "\n",
    "epochs = 10  # Changed from 10 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac4b3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to check: early stopping\n",
    "# set a relatively high number of epochs (e.g., 100 or 1000), and then \n",
    "# stop the training early if the model’s performance on a validation set \n",
    "# stops improving after a certain number of epochs\n",
    "\n",
    "# Define the early stopping criter\n",
    "patience = 3  # number of epochs to wait for improvement before stopping\n",
    "best_loss = None\n",
    "no_improve_count = 0\n",
    "\n",
    "# Initialize a list to hold the validation losses\n",
    "val_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "453c0d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/skqrkws14bs48tkyfrn2wnp80000gn/T/ipykernel_2345/2356349814.py:34: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'labels': torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 31.05 GB, other allocations: 5.20 GB, max allowed: 36.27 GB). Tried to allocate 48.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Add the validation loss for this epoch to the list\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1195\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1207\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CM3070_FP_Repo/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:226\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    229\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 31.05 GB, other allocations: 5.20 GB, max allowed: 36.27 GB). Tried to allocate 48.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = 0\n",
    "    for batch in val_data:  # use val_data instead of test_data\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "    # Add the validation loss for this epoch to the list\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_loss is None or val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        # Save the model\n",
    "        model_name = '_roberta-base_'\n",
    "        dataset_name = '_TruthSeeker2023_'\n",
    "        model_path = f\"_model_epoch{epoch}_valLoss{val_loss:.2f}_batchSize{train_data.batch_size}.pth\"\n",
    "        torch.save(model.state_dict(), dataset_name + model_name + model_path)\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "\n",
    "   # Stop training if there's no improvement for 'patience' epochs\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Stopping training after {epoch} epochs due to no improvement.\")\n",
    "        break\n",
    "\n",
    "# Plot the validation loss over each epoch\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# code for model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get the predictions for the test set\n",
    "y_pred = []\n",
    "for batch in test_data:\n",
    "    input_ids = batch['input_ids'].squeeze().to(device)\n",
    "    attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    y_pred.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9edb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (epoch)\n",
    "print (val_loss)\n",
    "print (train_data.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54486b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.defaults['lr'])\n",
    "print(optimizer.defaults['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13aeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = '_roberta-base_'\n",
    "dataset_name = '_TruthSeeker2023_'\n",
    "model_path = f\"_model_epoch{epoch}_valLoss{val_loss:.2f}_batchSize{train_data.batch_size}.pth\"\n",
    "torch.save(model.state_dict(), dataset_name + model_name + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e646c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb08318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
